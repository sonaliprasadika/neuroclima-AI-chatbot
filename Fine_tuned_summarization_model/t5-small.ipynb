{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39bc0ebe-ba4a-4cc0-b233-59abf4eab758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f3b145f-aba5-46c5-a585-a58f787517c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    country  \\\n",
      "0      United Arab Emirates   \n",
      "1                Kazakhstan   \n",
      "2              Saudi Arabia   \n",
      "3  United States of America   \n",
      "4                   Finland   \n",
      "\n",
      "                                  policy_description  \\\n",
      "0  The National Hydrogen Strategy 2050 aims to su...   \n",
      "1  The code regulates activities which may have a...   \n",
      "2  Saudi Arabia aims to increase the share of nat...   \n",
      "3              Focused on better lighting solutions.   \n",
      "4  Investment proposal for areas with coal-fired ...   \n",
      "\n",
      "                                             Summary  \\\n",
      "0  The National Hydrogen Strategy 2050 aims to po...   \n",
      "1  A code requiring the largest polluters to adop...   \n",
      "2  Saudi Arabia aims for 50% of electricity from ...   \n",
      "3  Focus on enhancing lighting solutions for ener...   \n",
      "4  Proposal for investing in regions with coal pl...   \n",
      "\n",
      "                             facebook/bart-large-cnn  \\\n",
      "0  The National Hydrogen Strategy 2050 aims to su...   \n",
      "1  The code regulates activities which may have a...   \n",
      "2  Saudi Arabia aims to increase the share of nat...   \n",
      "3                                                NaN   \n",
      "4                                                NaN   \n",
      "\n",
      "                                 google/flan-t5-base  \\\n",
      "0  The strategy aims to support low-carbon local ...   \n",
      "1                                The Code of Conduct   \n",
      "2  Saudi Arabia aims to increase the share of nat...   \n",
      "3                                                NaN   \n",
      "4                                                NaN   \n",
      "\n",
      "                                            T5-large  \n",
      "0  the national hydrogen Strategy 2050 aims to su...  \n",
      "1  the code regulates activities which may have a...  \n",
      "2  Saudi Arabia aims to increase the share of nat...  \n",
      "3                                                NaN  \n",
      "4                                                NaN  \n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the data\n",
    "data = pd.read_csv(\"summery_training.csv\", encoding=\"latin1\")\n",
    "print(data.head())\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "dataset = Dataset.from_pandas(data)\n",
    "\n",
    "# Step 2: Tokenize the data\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e61b35e8-823c-448c-a3d3-07fcf2aef2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 28.29 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Tokenize inputs (prefix input with \"summarize: \" for T5 models)\n",
    "    inputs = [\"summarize: \" + text for text in examples[\"policy_description\"]]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    # Tokenize targets (summary)\n",
    "    labels = tokenizer(\n",
    "        examples[\"Summary\"],\n",
    "        max_length=150,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2b9f244-ab95-429a-b5aa-b4be8c436037",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/myenv/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Load the T5 model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Step 4: Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./t5-summarization\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea51eb55-03ff-4ed4-8d1b-9e67474fd900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2808585/1963848570.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>15.053608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>13.826735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>13.389071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9, training_loss=14.587103949652779, metrics={'train_runtime': 45.7765, 'train_samples_per_second': 0.59, 'train_steps_per_second': 0.197, 'total_flos': 3654228639744.0, 'train_loss': 14.587103949652779, 'epoch': 3.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5: Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Step 6: Train and Evaluate\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a1babe5-4eba-4dce-9b16-1fc4852e6f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 13.389071464538574,\n",
       " 'eval_runtime': 2.7934,\n",
       " 'eval_samples_per_second': 3.222,\n",
       " 'eval_steps_per_second': 1.074,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ba124b2-881f-47ba-a4b1-ca8e327da7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./t5-summarization/tokenizer_config.json',\n",
       " './t5-summarization/special_tokens_map.json',\n",
       " './t5-summarization/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 7: Save the Model\n",
    "model.save_pretrained(\"./t5-summarization\")\n",
    "tokenizer.save_pretrained(\"./t5-summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82fb64c5-b53e-4bce-8795-0b2e8b7c6533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary: resolution 1-E/2017 establishes a discount of up to 20% on electricity prices. companies that want to benefit from reduced electicity price have to implement the ISO norm 50001 on energy management systems.\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model and tokenizer\n",
    "model_name = \"./t5-summarization\"  # Path to the saved model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate a summary for a given text\n",
    "def generate_summary(text):\n",
    "    # Prefix input with \"summarize: \" as required by T5\n",
    "    inputs = tokenizer(\n",
    "        \"summarize: \" + text,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Generate the summary\n",
    "    summary_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=150,\n",
    "        num_beams=4,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "\n",
    "    # Decode and return the summary\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Example usage\n",
    "policy_description = \"Resolution 1-E/2017 establishes a discount of up to 20% on electricity prices for energy-intensive industries. Provision 3/2018 (2018, as part of Joint Resolution 1-E/2017) Companies that want to benefit from reduced electicity price have to implement the ISO norm 50001 on energy management systems (i.e. develop a plan of action for energy management, establish targets for energy performance, and define indicators to monitor progress)\"\n",
    "summary = generate_summary(policy_description)\n",
    "print(\"Generated Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3ba254-a0fd-4d8b-ab2b-1977747158de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
