{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8facb0-b139-4d09-a884-00c886133cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install deepeval\n",
    "!pip3 install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd00ad30-8192-4f84-91c4-1367dde55279",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import AnswerRelevancyMetric, ContextualPrecisionMetric, ContextualRecallMetric, FaithfulnessMetric, HallucinationMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "import openai\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64edd6ee-1894-42f8-ba66-cba0bff0991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a7aea4a-613c-4ccc-95df-dd7ab50fe8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the CSV file\n",
    "df = pd.read_csv('deepeval_evaluation.csv', encoding='latin1')\n",
    "\n",
    "# Extract the relevant columns\n",
    "inputs = df['input'].tolist()\n",
    "actual_outputs = df['actual_output'].tolist()\n",
    "expected_outputs = df['expected_output'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fdec18d-49a2-427c-a5ee-11037aa19596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of rows\n",
    "n = 3  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05e9b9aa-c127-4b4b-ba0e-8dbe85866108",
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Answer Relevancy Metric\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Initialize the metric\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m metric \u001b[38;5;241m=\u001b[39m \u001b[43mAnswerRelevancyMetric\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Ensure you have access to this model\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_reason\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Create a list to hold test cases\u001b[39;00m\n\u001b[1;32m     11\u001b[0m test_cases \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/deepeval/metrics/answer_relevancy/answer_relevancy.py:38\u001b[0m, in \u001b[0;36mAnswerRelevancyMetric.__init__\u001b[0;34m(self, threshold, model, include_reason, async_mode, strict_mode, verbose_mode)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     30\u001b[0m     threshold: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     verbose_mode: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     36\u001b[0m ):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m strict_mode \u001b[38;5;28;01melse\u001b[39;00m threshold\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musing_native_model \u001b[38;5;241m=\u001b[39m \u001b[43minitialize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_model_name()\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_reason \u001b[38;5;241m=\u001b[39m include_reason\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/deepeval/metrics/utils.py:271\u001b[0m, in \u001b[0;36minitialize_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# Otherwise (the model is a string or None), we initialize a GPTModel and use as a native model\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGPTModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/deepeval/models/gpt_model.py:104\u001b[0m, in \u001b[0;36mGPTModel.__init__\u001b[0;34m(self, model, _openai_api_key, base_url, *args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 104\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/deepeval/models/base_model.py:35\u001b[0m, in \u001b[0;36mDeepEvalBaseLLM.__init__\u001b[0;34m(self, model_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_name: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m=\u001b[39m model_name\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/deepeval/models/gpt_model.py:157\u001b[0m, in \u001b[0;36mGPTModel.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CustomChatOpenAI(\n\u001b[1;32m    146\u001b[0m         model_name\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[1;32m    147\u001b[0m         openai_api_key\u001b[38;5;241m=\u001b[39mopenai_api_key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs,\n\u001b[1;32m    155\u001b[0m     )\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopenai_api_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_openai_api_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/langchain_core/load/serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:578\u001b[0m, in \u001b[0;36mBaseChatOpenAI.validate_environment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client \u001b[38;5;241m=\u001b[39m httpx\u001b[38;5;241m.\u001b[39mClient(proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopenai_proxy)\n\u001b[1;32m    577\u001b[0m     sync_specific \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client}\n\u001b[0;32m--> 578\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mclient_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msync_specific\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client:\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/openai/_client.py:110\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    108\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "# Answer Relevancy Metric\n",
    "\n",
    "# Initialize the metric\n",
    "metric = AnswerRelevancyMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4\",  # Ensure you have access to this model\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "# Create a list to hold test cases\n",
    "test_cases = []\n",
    "\n",
    "# Loop through the data to create test cases\n",
    "for i in range(n):\n",
    "    test_case = LLMTestCase(\n",
    "        input=inputs[i],\n",
    "        actual_output=actual_outputs[i]\n",
    "    )\n",
    "    test_cases.append(test_case)\n",
    "\n",
    "# Evaluate test cases in bulk\n",
    "evaluate(test_cases, [metric])\n",
    "\n",
    "# Optionally, print the scores and reasons\n",
    "for test_case in test_cases:\n",
    "    if hasattr(test_case, 'metrics') and test_case.metrics:  # Ensure metrics are available\n",
    "        print(f\"Test case input: {test_case.input}\")\n",
    "        print(f\"Answer Relevancy Score: {test_case.metrics[0].score}\")\n",
    "        print(f\"Reason: {test_case.metrics[0].reason}\")\n",
    "    else:\n",
    "        print(f\"Metrics not available for input: {test_case.input}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e632b6-9df1-479f-a428-ee0a71b03a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contextual Precision Metric\n",
    "metric = ContextualPrecisionMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4\",  # Ensure you have access to this model\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "# Create a list to hold test cases\n",
    "test_cases = []\n",
    "\n",
    "# Loop through the data to create 10 test cases\n",
    "for i in range(n):\n",
    "    test_case = LLMTestCase(\n",
    "        input=inputs[i],\n",
    "        actual_output=actual_outputs[i],\n",
    "        expected_output=expected_outputs[i],\n",
    "        retrieval_context=retrieval_context[i],\n",
    "        context=provided_context[i]\n",
    "    )\n",
    "    test_cases.append(test_case)\n",
    "\n",
    "# Evaluate test cases in bulk\n",
    "evaluate(test_cases, [metric])\n",
    "\n",
    "# Optionally, print the scores and reasons\n",
    "for test_case in test_cases:\n",
    "    if hasattr(test_case, 'metrics') and test_case.metrics:  # Ensure metrics are available\n",
    "        print(f\"Test case input: {test_case.input}\")\n",
    "        print(f\"Score: {test_case.metrics[0].score}\")\n",
    "        print(f\"Reason: {test_case.metrics[0].reason}\")\n",
    "    else:\n",
    "        print(f\"Metrics not available for input: {test_case.input}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a6c424-b1af-4ea7-a1d5-7fc22f0ddd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contextual Recall Metric\n",
    "metric = ContextualRecallMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4\",  # Ensure you have access to this model\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "# Create a list to hold test cases\n",
    "test_cases = []\n",
    "\n",
    "# Loop through the data to create test cases\n",
    "for i in range(n):\n",
    "    test_case = LLMTestCase(\n",
    "        input=inputs[i],\n",
    "        actual_output=actual_outputs[i],\n",
    "        expected_output=expected_outputs[i],\n",
    "        retrieval_context=retrieval_context[i],\n",
    "        context=provided_context[i]\n",
    "    )\n",
    "    test_cases.append(test_case)\n",
    "\n",
    "# Evaluate test cases in bulk\n",
    "evaluate(test_cases, [metric])\n",
    "\n",
    "# Optionally, print the scores and reasons\n",
    "for test_case in test_cases:\n",
    "    if hasattr(test_case, 'metrics') and test_case.metrics:  # Ensure metrics are available\n",
    "        print(f\"Test case input: {test_case.input}\")\n",
    "        print(f\"Recall Score: {test_case.metrics[0].score}\")\n",
    "        print(f\"Reason: {test_case.metrics[0].reason}\")\n",
    "    else:\n",
    "        print(f\"Metrics not available for input: {test_case.input}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b400b42d-c061-44f1-87f8-636088cb0e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FaithfulnessMetric\n",
    "metric = FaithfulnessMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4\",  # Ensure you have access to this model\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "# Create a list to hold test cases\n",
    "test_cases = []\n",
    "\n",
    "# Loop through the data to create test cases\n",
    "for i in range(n):\n",
    "    test_case = LLMTestCase(\n",
    "        input=inputs[i],\n",
    "        actual_output=actual_outputs[i],\n",
    "        retrieval_context=retrieval_context[i]\n",
    "    )\n",
    "    test_cases.append(test_case)\n",
    "\n",
    "# Evaluate test cases in bulk\n",
    "evaluate(test_cases, [metric])\n",
    "\n",
    "# Optionally, print the scores and reasons\n",
    "for test_case in test_cases:\n",
    "    if hasattr(test_case, 'metrics') and test_case.metrics:  # Ensure metrics are available\n",
    "        print(f\"Test case input: {test_case.input}\")\n",
    "        print(f\"Faithfulness Score: {test_case.metrics[0].score}\")\n",
    "        print(f\"Reason: {test_case.metrics[0].reason}\")\n",
    "    else:\n",
    "        print(f\"Metrics not available for input: {test_case.input}\")\n",
    "    print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced8cb92-977b-4380-8e22-6f68c3af1072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hallucination Metric\n",
    "metric = HallucinationMetric(threshold=0.5)\n",
    "\n",
    "# Create a list to hold test cases\n",
    "test_cases = []\n",
    "\n",
    "# Loop through the data to create test cases\n",
    "for i in range(n):\n",
    "    test_case = LLMTestCase(\n",
    "        input=inputs[i],\n",
    "        actual_output=actual_outputs[i],\n",
    "        expected_output=expected_outputs[i],\n",
    "        context=provided_context[i]\n",
    "    )\n",
    "    test_cases.append(test_case)\n",
    "\n",
    "# Evaluate test cases in bulk\n",
    "evaluate(test_cases, [metric])\n",
    "\n",
    "# Optionally, print the scores and reasons\n",
    "for test_case in test_cases:\n",
    "    if hasattr(test_case, 'metrics') and test_case.metrics:  # Ensure metrics are available\n",
    "        print(f\"Test case input: {test_case.input}\")\n",
    "        print(f\"Hallucination Score: {test_case.metrics[0].score}\")\n",
    "        print(f\"Reason: {test_case.metrics[0].reason}\")\n",
    "    else:\n",
    "        print(f\"Metrics not available for input: {test_case.input}\")\n",
    "    print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
