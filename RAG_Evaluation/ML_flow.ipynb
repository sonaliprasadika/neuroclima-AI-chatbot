{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fea8a2de-be5e-4407-b3fb-36927d6a7224",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlflow in /home/ubuntu/myenv/lib/python3.12/site-packages (2.19.0)\n",
      "Requirement already satisfied: mlflow-skinny==2.19.0 in /home/ubuntu/myenv/lib/python3.12/site-packages (from mlflow) (2.19.0)\n",
      "Requirement already satisfied: Flask<4 in /home/ubuntu/myenv/lib/python3.12/site-packages (from mlflow) (3.1.0)\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /home/ubuntu/myenv/lib/python3.12/site-packages (from mlflow) (3.1.5)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in /home/ubuntu/myenv/lib/python3.12/site-packages (from mlflow) (1.14.0)\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in /home/ubuntu/myenv/lib/python3.12/site-packages (from mlflow) (7.1.0)\n",
      "Requirement already satisfied: graphene<4 in /home/ubuntu/myenv/lib/python3.12/site-packages (from mlflow) (3.4.3)\n",
      "Requirement already satisfied: gunicorn<24 in /home/ubuntu/myenv/lib/python3.12/site-packages (from mlflow) (23.0.0)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in /home/ubuntu/myenv/lib/python3.12/site-packages (from mlflow) (3.7)\n",
      "Requirement already satisfied: matplotlib<4 in /home/ubuntu/myenv/lib/python3.12/site-packages (from mlflow) (3.10.0)\n",
      "Requirement already satisfied: numpy<3 in /home/ubuntu/myenv/lib/python3.12/site-packages (from mlflow) (1.26.4)\n",
      "Requirement already satisfied: pandas<3 in /home/ubuntu/myenv/lib/python3.12/site-packages (from mlflow) (2.2.3)\n",
      "Requirement already satisfied: pyarrow<19,>=4.0.0 in /home/ubuntu/myenv/lib/python3.12/site-packages (from mlflow) (18.1.0)\n",
      "Requirement already satisfied: scikit-learn<2 in /home/ubuntu/myenv/lib/python3.12/site-packages (from mlflow) (1.6.0)\n",
      "Requirement already satisfied: scipy<2 in /home/ubuntu/myenv/lib/python3.12/site-packages (from mlflow) (1.13.1)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /home/ubuntu/myenv/lib/python3.12/site-packages (from mlflow) (2.0.36)\n",
      "Requirement already satisfied: cachetools<6,>=5.0.0 in /home/ubuntu/myenv/lib/python3.12/site-packages (from mlflow-skinny==2.19.0->mlflow) (5.5.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in /home/ubuntu/myenv/lib/python3.12/site-packages (from mlflow-skinny==2.19.0->mlflow) (8.1.8)\n",
      "Requirement already satisfied: cloudpickle<4 in /home/ubuntu/myenv/lib/python3.12/site-packages (from mlflow-skinny==2.19.0->mlflow) (3.1.0)\n",
      "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /home/ubuntu/myenv/lib/python3.12/site-packages (from mlflow-skinny==2.19.0->mlflow) (0.40.0)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /home/ubuntu/myenv/lib/python3.12/site-packages (from mlflow-skinny==2.19.0->mlflow) (3.1.43)\n",
      "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /home/ubuntu/myenv/lib/python3.12/site-packages (from mlflow-skinny==2.19.0->mlflow) (8.4.0)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /home/ubuntu/myenv/lib/python3.12/site-packages (from mlflow-skinny==2.19.0->mlflow) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /home/ubuntu/myenv/lib/python3.12/site-packages (from mlflow-skinny==2.19.0->mlflow) (1.27.0)\n",
      "Requirement already satisfied: packaging<25 in /home/ubuntu/myenv/lib/python3.12/site-packages (from mlflow-skinny==2.19.0->mlflow) (24.2)\n",
      "Requirement already satisfied: protobuf<6,>=3.12.0 in /home/ubuntu/myenv/lib/python3.12/site-packages (from mlflow-skinny==2.19.0->mlflow) (4.25.5)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in /home/ubuntu/myenv/lib/python3.12/site-packages (from mlflow-skinny==2.19.0->mlflow) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in /home/ubuntu/myenv/lib/python3.12/site-packages (from mlflow-skinny==2.19.0->mlflow) (2.32.3)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /home/ubuntu/myenv/lib/python3.12/site-packages (from mlflow-skinny==2.19.0->mlflow) (0.5.3)\n",
      "Requirement already satisfied: Mako in /home/ubuntu/myenv/lib/python3.12/site-packages (from alembic!=1.10.0,<2->mlflow) (1.3.8)\n",
      "Requirement already satisfied: typing-extensions>=4 in /home/ubuntu/myenv/lib/python3.12/site-packages (from alembic!=1.10.0,<2->mlflow) (4.12.2)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /home/ubuntu/myenv/lib/python3.12/site-packages (from docker<8,>=4.0.0->mlflow) (2.3.0)\n",
      "Requirement already satisfied: Werkzeug>=3.1 in /home/ubuntu/myenv/lib/python3.12/site-packages (from Flask<4->mlflow) (3.1.3)\n",
      "Requirement already satisfied: itsdangerous>=2.2 in /home/ubuntu/myenv/lib/python3.12/site-packages (from Flask<4->mlflow) (2.2.0)\n",
      "Requirement already satisfied: blinker>=1.9 in /home/ubuntu/myenv/lib/python3.12/site-packages (from Flask<4->mlflow) (1.9.0)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in /home/ubuntu/myenv/lib/python3.12/site-packages (from graphene<4->mlflow) (3.2.5)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /home/ubuntu/myenv/lib/python3.12/site-packages (from graphene<4->mlflow) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /home/ubuntu/myenv/lib/python3.12/site-packages (from graphene<4->mlflow) (2.9.0.post0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/myenv/lib/python3.12/site-packages (from Jinja2<4,>=2.11->mlflow) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ubuntu/myenv/lib/python3.12/site-packages (from matplotlib<4->mlflow) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ubuntu/myenv/lib/python3.12/site-packages (from matplotlib<4->mlflow) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ubuntu/myenv/lib/python3.12/site-packages (from matplotlib<4->mlflow) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ubuntu/myenv/lib/python3.12/site-packages (from matplotlib<4->mlflow) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /home/ubuntu/myenv/lib/python3.12/site-packages (from matplotlib<4->mlflow) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ubuntu/myenv/lib/python3.12/site-packages (from matplotlib<4->mlflow) (3.2.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/myenv/lib/python3.12/site-packages (from pandas<3->mlflow) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ubuntu/myenv/lib/python3.12/site-packages (from pandas<3->mlflow) (2024.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ubuntu/myenv/lib/python3.12/site-packages (from scikit-learn<2->mlflow) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ubuntu/myenv/lib/python3.12/site-packages (from scikit-learn<2->mlflow) (3.5.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ubuntu/myenv/lib/python3.12/site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.1)\n",
      "Requirement already satisfied: google-auth~=2.0 in /home/ubuntu/myenv/lib/python3.12/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (2.37.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/ubuntu/myenv/lib/python3.12/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.19.0->mlflow) (4.0.11)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ubuntu/myenv/lib/python3.12/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.19.0->mlflow) (3.21.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /home/ubuntu/myenv/lib/python3.12/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.19.0->mlflow) (1.2.15)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in /home/ubuntu/myenv/lib/python3.12/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.19.0->mlflow) (0.48b0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ubuntu/myenv/lib/python3.12/site-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/myenv/lib/python3.12/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.19.0->mlflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/myenv/lib/python3.12/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.19.0->mlflow) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/myenv/lib/python3.12/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.19.0->mlflow) (2024.12.14)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/ubuntu/myenv/lib/python3.12/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.19.0->mlflow) (1.17.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/ubuntu/myenv/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.19.0->mlflow) (5.0.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ubuntu/myenv/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ubuntu/myenv/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (4.9)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /home/ubuntu/myenv/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (0.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10195a9f-c116-4a1d-a78c-f47f32b03c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mlflow\n",
    "from mlflow.metrics.genai import faithfulness, relevance\n",
    "import openai\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e565626-575f-4256-a984-88b5b216c938",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] =  \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1b719ff-3c48-47a3-a870-a9313278766a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               input  \\\n",
      "0       What is the National Hydrogen Strategy 2050?   \n",
      "1                    What is the Environmental Code?   \n",
      "2  What are the targets of the Energy Policy Fram...   \n",
      "3                       What is the Electricity Act?   \n",
      "4  What is the Vision 2030 strategy for Saudi Ara...   \n",
      "\n",
      "                                     expected_output  \\\n",
      "0  The National Hydrogen Strategy 2050 aims to su...   \n",
      "1  The Environmental Code regulates activities im...   \n",
      "2  The Energy Policy Framework aims to reduce ene...   \n",
      "3  The Electricity Act sets regulations on energy...   \n",
      "4  Saudi Arabia aims to increase the share of nat...   \n",
      "\n",
      "                                   retrieval_context  \\\n",
      "0  ten points that the government will place effo...   \n",
      "1  a number of policies affecting road traffic ha...   \n",
      "2  the UK's 2nd Energy Efficiency Action Plan (EE...   \n",
      "3  a number of policies affecting road traffic ha...   \n",
      "4  ten points that the government will place effo...   \n",
      "\n",
      "                                       actual_output  \n",
      "0  The National Hydrogen Strategy 2050 is a long-...  \n",
      "1  The Environmental Code is a legal framework de...  \n",
      "2  The targets of the Energy Policy Framework (PO...  \n",
      "3  The Electricity Act, specifically the version ...  \n",
      "4  The Vision 2030 strategy for Saudi Arabia's en...  \n"
     ]
    }
   ],
   "source": [
    "# Load the data from the CSV file\n",
    "df = pd.read_csv('RAG_evaluation.csv', encoding='latin1')\n",
    "print(df.head())\n",
    "# Extract the relevant columns\n",
    "inputs = df['input'].tolist()\n",
    "actual_outputs = df['actual_output'].tolist()\n",
    "expected_outputs = df['expected_output'].tolist()\n",
    "retrieval_context = df['retrieval_context'].apply(lambda x: [x] if isinstance(x, str) else None).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63c15869-8c87-4783-a91e-a7544b3e71ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluationMetric(name=relevance, greater_is_better=True, long_name=relevance, version=v1, metric_details=\n",
      "Task:\n",
      "You must return the following fields in your response in two lines, one below the other:\n",
      "score: Your numerical score for the model's relevance based on the rubric\n",
      "justification: Your reasoning about the model's relevance score\n",
      "\n",
      "You are an impartial judge. You will be given an input that was sent to a machine\n",
      "learning model, and you will be given an output that the model produced. You\n",
      "may also be given additional information that was used by the model to generate the output.\n",
      "\n",
      "Your task is to determine a numerical score called relevance based on the input and output.\n",
      "A definition of relevance and a grading rubric are provided below.\n",
      "You must use the grading rubric to determine your score. You must also justify your score.\n",
      "\n",
      "Examples could be included below for reference. Make sure to use them as references and to\n",
      "understand them before completing the task.\n",
      "\n",
      "Input:\n",
      "{input}\n",
      "\n",
      "Output:\n",
      "{output}\n",
      "\n",
      "{grading_context_columns}\n",
      "\n",
      "Metric definition:\n",
      "Relevance encompasses the appropriateness, significance, and applicability of the output with respect to both the input and context. Scores should reflect the extent to which the output directly addresses the question provided in the input, given the provided context.\n",
      "\n",
      "Grading rubric:\n",
      "Relevance: Below are the details for different scores:- Score 1: The output doesn't mention anything about the question or is completely irrelevant to the provided context.\n",
      "- Score 2: The output provides some relevance to the question and is somehow related to the provided context.\n",
      "- Score 3: The output mostly answers the question and is largely consistent with the provided context.\n",
      "- Score 4: The output answers the question and is consistent with the provided context.\n",
      "- Score 5: The output answers the question comprehensively using the provided context.\n",
      "\n",
      "Examples:\n",
      "\n",
      "Example Input:\n",
      "How is MLflow related to Databricks?\n",
      "\n",
      "Example Output:\n",
      "Databricks is a data engineering and analytics platform designed to help organizations process and analyze large amounts of data. Databricks is a company specializing in big data and machine learning solutions.\n",
      "\n",
      "Additional information used by the model:\n",
      "key: context\n",
      "value:\n",
      "MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is designed to address the challenges that data scientists and machine learning engineers face when developing, training, and deploying machine learning models.\n",
      "\n",
      "Example score: 2\n",
      "Example justification: The output provides relevant information about Databricks, mentioning it as a company specializing in big data and machine learning solutions. However, it doesn't directly address how MLflow is related to Databricks, which is the specific question asked in the input. Therefore, the output is only somewhat related to the provided context.\n",
      "        \n",
      "\n",
      "Example Input:\n",
      "How is MLflow related to Databricks?\n",
      "\n",
      "Example Output:\n",
      "MLflow is a product created by Databricks to enhance the efficiency of machine learning processes.\n",
      "\n",
      "Additional information used by the model:\n",
      "key: context\n",
      "value:\n",
      "MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is designed to address the challenges that data scientists and machine learning engineers face when developing, training, and deploying machine learning models.\n",
      "\n",
      "Example score: 4\n",
      "Example justification: The output provides a relevant and accurate statement about the relationship between MLflow and Databricks. While it doesn't provide extensive detail, it still offers a substantial and meaningful response. To achieve a score of 5, the response could be further improved by providing additional context or details about how MLflow specifically functions within the Databricks ecosystem.\n",
      "        \n",
      "\n",
      "You must return the following fields in your response in two lines, one below the other:\n",
      "score: Your numerical score for the model's relevance based on the rubric\n",
      "justification: Your reasoning about the model's relevance score\n",
      "\n",
      "Do not add additional new lines. Do not add any other fields.\n",
      "    )\n",
      "EvaluationMetric(name=faithfulness, greater_is_better=True, long_name=faithfulness, version=v1, metric_details=\n",
      "Task:\n",
      "You must return the following fields in your response in two lines, one below the other:\n",
      "score: Your numerical score for the model's faithfulness based on the rubric\n",
      "justification: Your reasoning about the model's faithfulness score\n",
      "\n",
      "You are an impartial judge. You will be given an input that was sent to a machine\n",
      "learning model, and you will be given an output that the model produced. You\n",
      "may also be given additional information that was used by the model to generate the output.\n",
      "\n",
      "Your task is to determine a numerical score called faithfulness based on the input and output.\n",
      "A definition of faithfulness and a grading rubric are provided below.\n",
      "You must use the grading rubric to determine your score. You must also justify your score.\n",
      "\n",
      "Examples could be included below for reference. Make sure to use them as references and to\n",
      "understand them before completing the task.\n",
      "\n",
      "Input:\n",
      "{input}\n",
      "\n",
      "Output:\n",
      "{output}\n",
      "\n",
      "{grading_context_columns}\n",
      "\n",
      "Metric definition:\n",
      "Faithfulness is only evaluated with the provided output and provided context, please ignore the provided input entirely when scoring faithfulness. Faithfulness assesses how much of the provided output is factually consistent with the provided context. A higher score indicates that a higher proportion of claims present in the output can be derived from the provided context. Faithfulness does not consider how much extra information from the context is not present in the output.\n",
      "\n",
      "Grading rubric:\n",
      "Faithfulness: Below are the details for different scores:\n",
      "- Score 1: None of the claims in the output can be inferred from the provided context.\n",
      "- Score 2: Some of the claims in the output can be inferred from the provided context, but the majority of the output is missing from, inconsistent with, or contradictory to the provided context.\n",
      "- Score 3: Half or more of the claims in the output can be inferred from the provided context.\n",
      "- Score 4: Most of the claims in the output can be inferred from the provided context, with very little information that is not directly supported by the provided context.\n",
      "- Score 5: All of the claims in the output are directly supported by the provided context, demonstrating high faithfulness to the provided context.\n",
      "\n",
      "Examples:\n",
      "\n",
      "Example Output:\n",
      "Databricks is a company that specializes in big data and machine learning solutions. MLflow has nothing to do with Databricks. MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle.\n",
      "\n",
      "Additional information used by the model:\n",
      "key: context\n",
      "value:\n",
      "MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is designed to address the challenges that data scientists and machine learning engineers face when developing, training, and deploying machine learning models.\n",
      "\n",
      "Example score: 2\n",
      "Example justification: The output claims that \"MLflow has nothing to do with Databricks\" which is contradictory to the provided context that states \"It was developed by Databricks\". This is a major inconsistency. However, the output correctly identifies that \"MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle\" and \"Databricks is a company that specializes in big data and machine learning solutions\", which are both supported by the context. Therefore, some of the claims in the output can be inferred from the provided context, but the majority of the output is inconsistent with the provided context, leading to a faithfulness score of 2.\n",
      "        \n",
      "\n",
      "Example Output:\n",
      "Databricks is a company that specializes in big data and machine learning solutions.\n",
      "\n",
      "Additional information used by the model:\n",
      "key: context\n",
      "value:\n",
      "MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is designed to address the challenges that data scientists and machine learning engineers face when developing, training, and deploying machine learning models.\n",
      "\n",
      "Example score: 5\n",
      "Example justification: The output states that \"Databricks is a company that specializes in big data and machine learning solutions.\" This claim is directly supported by the context, which states \"It was developed by Databricks, a company that specializes in big data and machine learning solutions.\" Therefore, the faithfulness score is 5 as all the claims in the output are directly supported by the provided context.\"\n",
      "        \n",
      "\n",
      "You must return the following fields in your response in two lines, one below the other:\n",
      "score: Your numerical score for the model's faithfulness based on the rubric\n",
      "justification: Your reasoning about the model's faithfulness score\n",
      "\n",
      "Do not add additional new lines. Do not add any other fields.\n",
      "    )\n"
     ]
    }
   ],
   "source": [
    "# Create a Pandas DataFrame for evaluation\n",
    "eval_df = pd.DataFrame({\n",
    "    \"inputs\": inputs,\n",
    "    \"outputs\": actual_outputs,\n",
    "    \"ground_truth\": expected_outputs,\n",
    "    \"context\": retrieval_context,  # Rename retrieval_context to context\n",
    "})\n",
    "\n",
    "# Convert to an MLflow-compatible dataset\n",
    "eval_dataset = mlflow.data.from_pandas(\n",
    "    eval_df, predictions=\"outputs\", targets=\"ground_truth\"\n",
    ")\n",
    "\n",
    "# Define relevance and faithfulness metrics\n",
    "relevance_metric = relevance(model=\"openai:/gpt-4o-mini\")\n",
    "faithfulness_metric = faithfulness(model=\"openai:/gpt-4o-mini\")\n",
    "\n",
    "print(relevance_metric)\n",
    "print(faithfulness_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d52a4dd4-1ada-4332-979b-7fc45dd6f559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/29 20:53:10 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "2024/12/29 20:53:10 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2024/12/29 20:53:10 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'toxicity' at index 1 in the `extra_metrics` parameter because it returned None.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.70s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.55s/it]\n",
      "2024/12/29 20:53:14 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2024/12/29 20:53:14 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'toxicity' at index 1 in the `extra_metrics` parameter because it returned None.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 82/82 [00:12<00:00,  6.81it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 82/82 [00:21<00:00,  3.90it/s]\n",
      "Downloading artifacts: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 897.37it/s]\n",
      "Downloading artifacts: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 834.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results saved to rag_evaluation_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"RAG_Evaluation\"):\n",
    "    # Run evaluation using the default evaluator and additional metrics\n",
    "    results = mlflow.evaluate(\n",
    "        data=eval_dataset,\n",
    "        model_type=\"question-answering\",\n",
    "        evaluators=[\"default\"],  # Use default evaluator\n",
    "        extra_metrics=[relevance_metric, faithfulness_metric],\n",
    "    )\n",
    "\n",
    "    # Extract the relevance and faithfulness scores from the results\n",
    "    relevance_score = results.metrics.get(\"relevance_metric\", None)\n",
    "    faithfulness_score = results.metrics.get(\"faithfulness\", None)\n",
    "\n",
    "    # Convert the evaluation results table to a DataFrame\n",
    "    results_table = results.tables.get(\"eval_results_table\")\n",
    "    if results_table is not None:\n",
    "        results_file_path = \"rag_evaluation_results.csv\"\n",
    "        results_table.to_csv(results_file_path, index=False)\n",
    "        print(f\"Evaluation results saved to {results_file_path}\")\n",
    "    else:\n",
    "        print(\"No results table available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2cd78f3-fc19-4312-ab49-825c731a0f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Names: ['inputs', 'context', 'ground_truth', 'outputs', 'token_count', 'flesch_kincaid_grade_level/v1/score', 'ari_grade_level/v1/score', 'relevance/v1/score', 'relevance/v1/justification', 'faithfulness/v1/score', 'faithfulness/v1/justification']\n"
     ]
    }
   ],
   "source": [
    "# Load the results table (CSV file) into a Pandas DataFrame\n",
    "results_file_path = \"rag_evaluation_results.csv\"\n",
    "df = pd.read_csv(results_file_path)\n",
    "# Print the column names\n",
    "print(\"Column Names:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43dcddb5-d104-4c5b-b5f9-fca44db9d553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Relevance Score from CSV: 4.170731707317073\n",
      "Average Faithfulness Score from CSV: 3.6951219512195124\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average value of the \"relevance_metric\" and \"faithfulness\" columns\n",
    "if \"relevance/v1/score\" in df.columns:\n",
    "    average_relevance_score = df[\"relevance/v1/score\"].mean()\n",
    "    print(f\"Average Relevance Score from CSV: {average_relevance_score}\")\n",
    "else:\n",
    "    print(\"Relevance metric column not found in the CSV file.\")\n",
    "\n",
    "if \"faithfulness/v1/score\" in df.columns:\n",
    "    average_faithfulness_score = df[\"faithfulness/v1/score\"].mean()\n",
    "    print(f\"Average Faithfulness Score from CSV: {average_faithfulness_score}\")\n",
    "else:\n",
    "    print(\"Faithfulness column not found in the CSV file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720ce52c-92f0-48b2-9faa-4a17e0ffbc1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
